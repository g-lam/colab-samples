{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/g-lam/colab-samples/blob/main/Text-to-Text/Llama_2_GGUF_CTransformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-hlb3r0vemt",
        "outputId": "258b3b4b-c488-47f5-8ccc-032c71cd7334"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIBbLjHawTlw",
        "outputId": "9d269659-2299-4ed8-ec61-4300aafe02d4"
      },
      "outputs": [],
      "source": [
        "!lscpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSxX-VO5ya0c",
        "outputId": "a745c12b-10d2-4f0b-e611-c3233929ab0f"
      },
      "outputs": [],
      "source": [
        "!CT_CUBLAS=1 pip install ctransformers --no-binary ctransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ctransformers import AutoModelForCausalLM\n",
        "\n",
        "config = {'max_new_tokens': 256, 'repetition_penalty': 1.1, 'temperature': 0.1, 'stream': True}\n",
        "\n",
        "models = [\n",
        "    {\n",
        "        'name': \"TheBloke/Llama-2-7b-Chat-GGUF\",\n",
        "        'file': \"llama-2-7b-chat.Q4_K_M.gguf\",\n",
        "        'layers': 110 #110 for 7b, 130 for 13b\n",
        "    },\n",
        "    {\n",
        "        'name': \"TheBloke/Llama-2-13B-chat-GGUF\",\n",
        "        'file': \"llama-2-13b-chat.Q4_K_M.gguf\",\n",
        "        'layers': 130 #110 for 7b, 130 for 13b\n",
        "    },\n",
        "    {\n",
        "        'name': \"TheBloke/Llama-2-70B-chat-GGUF\",\n",
        "        'file': \"llama-2-70b-chat.Q5_K_S.gguf\",\n",
        "        'layers': 150 #110 for 7b, 130 for 13b\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4zXrO1Ehv3N"
      },
      "outputs": [],
      "source": [
        "model_index = 0\n",
        "\n",
        "llm = AutoModelForCausalLM.from_pretrained(models[model_index]['name'],\n",
        "                                           model_type=\"llama\",\n",
        "                                           model_file=models[model_index]['file'],\n",
        "                                           #lib='avx2', #for cpu use\n",
        "                                           gpu_layers=models[model_index]['layers'],\n",
        "                                           **config\n",
        "                                           )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnWlnPuV4K29"
      },
      "source": [
        "## LLAMA-2 Model Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA7q--qmxJT0"
      },
      "source": [
        "### Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHL-6-iGzgO5"
      },
      "outputs": [],
      "source": [
        "prompt=\"\"\"Write a poem to help me remember the first 10 elements on the periodic table, giving each\n",
        "element its own line.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1CDKtF_w7qt"
      },
      "outputs": [],
      "source": [
        "tokens = llm.tokenize(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVKptuwQxHBR",
        "outputId": "0b17f8da-d4a9-4e45-902c-ff564bb2fcc6"
      },
      "outputs": [],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgTA3z-9yLiG"
      },
      "source": [
        "### Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "wItUOIoTxskA",
        "outputId": "489b6759-9aa2-4c06-f395-90da4e3bd66d"
      },
      "outputs": [],
      "source": [
        "# 'pipeline' execution\n",
        "llm(prompt, stream=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "TVUiwWls3uWC",
        "outputId": "98fd3994-a31f-416f-9d3b-f7bf86625fb0"
      },
      "outputs": [],
      "source": [
        "prompt2 = \"\"\"Quando e por quem o Brasil foi descoberto?\"\"\"\n",
        "llm(prompt2, stream=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3po3AlayOAA"
      },
      "source": [
        "### Generate with stream execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQkt0pnXxhlk",
        "outputId": "e3eeab64-cc4a-44d5-8b88-93887f878410"
      },
      "outputs": [],
      "source": [
        "# LlAMA-2-7b-chat execution\n",
        "import time\n",
        "start = time.time()\n",
        "NUM_TOKENS=0\n",
        "print('-'*4+'Start Generation'+'-'*4)\n",
        "for token in llm.generate(tokens):\n",
        "    print(llm.detokenize(token), end='', flush=True)\n",
        "    NUM_TOKENS+=1\n",
        "time_generate = time.time() - start\n",
        "print('\\n')\n",
        "print('-'*4+'End Generation'+'-'*4)\n",
        "print(f'Num of generated tokens: {NUM_TOKENS}')\n",
        "print(f'Time for complete generation: {time_generate}s')\n",
        "print(f'Tokens per secound: {NUM_TOKENS/time_generate}')\n",
        "print(f'Time per token: {(time_generate/NUM_TOKENS)*1000}ms')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sstVjp7xALyv"
      },
      "source": [
        "## LangChain Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC_HpPtSAR_x"
      },
      "source": [
        "https://python.langchain.com/docs/ecosystem/integrations/ctransformers"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNBiOVdvNcepnMtwRDooVYS",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
